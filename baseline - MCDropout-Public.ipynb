{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d7fe926",
   "metadata": {},
   "source": [
    "# Baseline Solution - Monte Carlo Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcaafdf",
   "metadata": {},
   "source": [
    "## This notebook documents the baseline solution for ADC 2022. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c438b",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Our challenge is to provide 1. an error estimates (for Light Track) and/or 2. a conditional probability distribution (for Regular Track) for each target (6 in total) given an observation from the Ariel Space Telescope. The light track encourages a natural progression to the regular track. Participants are welcomed to join either or both tracks as they see fit. \n",
    "\n",
    "Depending on the information content of the observation and the associated observation noise (which is a function of the instrument and the planetary system), the resultant error bounds on each target and their joint conditional distribution will be different.\n",
    "\n",
    "There are many directions you can take to tackle the problem on hand. We would like to get you started with our baseline solution. \n",
    "\n",
    "Spectroscopic data alone are usually informative enough to provide a reasonable estiamte on the targets. After all, the trough and peaks in the spectra encoded information about the relative abundance of each gaseous species (see [Yip et al.](https://iopscience.iop.org/article/10.3847/1538-3881/ac1744>) ). The supplementary information also helps to better constrain some of the phyiscal quantities (see our discussion [here](https://www.ariel-datachallenge.space/ML/documentation/about) if you want to learn about the underlying physics :) , but I shall leave that to you. \n",
    "\n",
    "The baseline solution trains a CNN to output a deterministic estimate for each atmospheric target. At inference time, the network is made to produce probabilistic output by activating the dropout layers in the network (Monte Carlo Dropout, [Gal et al. 2016](https://arxiv.org/abs/1506.02142)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61537610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import h5py\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense, Reshape, Input, Concatenate, BatchNormalization, Dropout, Conv1D,Flatten,MaxPooling1D\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm\n",
    "from helper import *\n",
    "from preprocessing import *\n",
    "from submit_format import *\n",
    "from metric_light_track import *\n",
    "from metric_regular_track import *\n",
    "\n",
    "from MCDropout import MC_Convtrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10834d90",
   "metadata": {},
   "source": [
    "### Fix seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace7f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b708058f",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RJUP = 69911000\n",
    "MJUP = 1.898e27\n",
    "RSOL = 696340000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a8402",
   "metadata": {},
   "source": [
    "## Read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de11f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = '/TrainingData/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '/TestData/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef935b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_GT_path = os.path.join(training_path, 'Ground Truth Package')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd824849",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_training_data = h5py.File(os.path.join(training_path,'SpectralData.hdf5'),\"r\")\n",
    "aux_training_data = pd.read_csv(os.path.join(training_path,'AuxillaryTable.csv'))\n",
    "soft_label_data = pd.read_csv(os.path.join(training_GT_path, 'FM_Parameter_Table.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9b068d",
   "metadata": {},
   "source": [
    "## Extract Spectral data\n",
    "Spectral data lives in a h5py format, which is useful for navigating different cases, but their format makes it difficult to bulk manage them. The helper function helps to transform the h5py file into a matrix of size N x 52 x 4\n",
    "where N is the number of training examples, 52 is the number of wavelength channels and 4 is the observation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e90f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_matrix = to_observed_matrix(spectral_training_data,aux_training_data)\n",
    "print(\"spectral matrix shape:\", spec_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d708b807",
   "metadata": {},
   "source": [
    "# Visualising a single spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b82614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_spectrum(spectrum):\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    plt.errorbar(x=spectrum[:,0], y= spectrum[:,1], yerr=spectrum[:,2] )\n",
    "    ## usually we visualise it in log-scale\n",
    "    plt.xscale('log')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df6f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_spectrum(spec_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets look at another one\n",
    "visualise_spectrum(spec_matrix[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50d49f8",
   "metadata": {},
   "source": [
    "it is immediately apparent that the average transit depth between two spectra can change for an order of magnitude difference. The magnitude of the uncertainty can also change accordingly ( and is a function of the planetary system, brightness of the host star and instrument response function). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6386b02d",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68c8aa3",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9946dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat = 5\n",
    "threshold = 0.8 ## for train valid split.\n",
    "N = 5000 # train on the first 5000 data instances, remember only the first 26k examples are labelled, the rest are unlabelled!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49ae719",
   "metadata": {},
   "source": [
    "We can safely discard wlgrid (wavelength grid) and wlwidth (width of wavelength) since they are unchanged in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510cff82",
   "metadata": {},
   "source": [
    "### Extract Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99e956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract the noise\n",
    "noise = spec_matrix[:N,:,2]\n",
    "## We will incorporate the noise profile into the observed spectrum by treating the noise as Gaussian noise.\n",
    "spectra = spec_matrix[:N,:,1]\n",
    "wl_channels = len(spec_matrix[0,:,0])\n",
    "global_mean = np.mean(spectra)\n",
    "global_std = np.std(spectra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f973dbe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d9e0795",
   "metadata": {},
   "source": [
    "### Add additional features - radius of the star and the planet\n",
    "Most of the time we know something about the planetary system before we even attempt to make an observation (we cant just point randomly with a multi-million euros instrument!). Some of these auxillary data may be useful for retrieval, here we are only using the radius of the star and the planet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890852c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add Rstar and Rplanet\n",
    "radii = aux_training_data[['star_radius_m', 'planet_radius_m']]\n",
    "## we would prefer to use Rsol and Rjup \n",
    "radii['star_radius'] = radii['star_radius_m']/RSOL\n",
    "radii['planet_radius'] = radii['planet_radius_m']/RJUP\n",
    "radii = radii.drop(['star_radius_m', 'planet_radius_m'],axis=1)\n",
    "radii = radii.iloc[:N, :]\n",
    "mean_radii = radii.mean()\n",
    "stdev = radii.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b5ac2",
   "metadata": {},
   "source": [
    "### get target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749830e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = ['planet_temp','log_H2O','log_CO2','log_CH4','log_CO','log_NH3']\n",
    "targets = soft_label_data.iloc[:N][target_labels]\n",
    "num_targets = targets.shape[1]\n",
    "targets_mean = targets.mean()\n",
    "targets_std = targets.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe76dd",
   "metadata": {},
   "source": [
    "## Train/valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e265d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.random.rand(len(spectra)) < threshold\n",
    "training_spectra, training_radii,training_targets, training_noise = spectra[ind],radii[ind],targets[ind], noise[ind]\n",
    "valid_spectra, valid_radii, valid_targets = spectra[~ind],radii[~ind],targets[~ind]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a9501",
   "metadata": {},
   "source": [
    "## Augment the dataset with noise (create multiple instances)\n",
    "Observational noise from Ariel forms an important part of the challenge, any model must recognise that the observation are not absolute measurement and could vary (according to the uncertainty), as that will affect the uncertainty associated with our atmospheric targets. Here we try to incorporate these information by augmenting the data with the mean noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0393e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_spectra = augment_data_with_noise(training_spectra, training_noise, repeat)\n",
    "aug_radii = np.tile(training_radii.values,(repeat,1))\n",
    "aug_targets = np.tile(training_targets.values,(repeat,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee4036a",
   "metadata": {},
   "source": [
    "### Standardise the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787437e",
   "metadata": {},
   "source": [
    "### spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcb9ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## standardise the input using global mean and stdev\n",
    "std_aug_spectra = standardise(aug_spectra, global_mean, global_std)\n",
    "std_aug_spectra = std_aug_spectra.reshape(-1, wl_channels)\n",
    "std_valid_spectra = standardise(valid_spectra, global_mean, global_std)\n",
    "std_valid_spectra = std_valid_spectra.reshape(-1, wl_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76378a24",
   "metadata": {},
   "source": [
    "### radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00714009",
   "metadata": {},
   "outputs": [],
   "source": [
    "## standardise\n",
    "std_aug_radii= standardise(aug_radii, mean_radii.values.reshape(1,-1), stdev.values.reshape(1,-1))\n",
    "std_valid_radii= standardise(valid_radii, mean_radii, stdev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51440d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9f848b1",
   "metadata": {},
   "source": [
    "### target\n",
    "We are asking the model to provide estimates for 6 atmospheric targets. In this example will be performing a supervised learning task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e863c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_aug_targets = standardise(aug_targets, targets_mean.values.reshape(1,-1), targets_std.values.reshape(1,-1))\n",
    "std_valid_targets = standardise(valid_targets, targets_mean, targets_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed42bc8",
   "metadata": {},
   "source": [
    "# Setup network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6e99c1",
   "metadata": {},
   "source": [
    "### hyperparameter settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 32\n",
    "lr= 1e-3\n",
    "epochs = 10\n",
    "filters = [32,64,128]\n",
    "dropout = 0.1\n",
    "# number of examples to generate in evaluation time (5000 is max for this competition)\n",
    "N_samples = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc9c85",
   "metadata": {},
   "source": [
    "We followed [Yip et al.](https://iopscience.iop.org/article/10.3847/1538-3881/ac1744>) and adopted a simple CNN structure and loss function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d9a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MC_Convtrainer(wl_channels,num_targets,dropout,filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ac6d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9fea61",
   "metadata": {},
   "source": [
    "### Compile model and Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ccab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compile model and run\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr),\n",
    "    loss='mse',)\n",
    "model.fit([std_aug_spectra,std_aug_radii], \n",
    "          std_aug_targets, \n",
    "          validation_data=([std_valid_spectra, std_valid_radii],std_valid_targets),\n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          shuffle=False,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3a53c1",
   "metadata": {},
   "source": [
    "### evaluate validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4361b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## select the corresponding GT for the validation data, and in the correct order.\n",
    "index= np.arange(len(ind))\n",
    "valid_index = index[~ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd8c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "##generate trace data using dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3559aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = N_samples\n",
    "y_pred_valid = np.zeros((instances, len(std_valid_spectra), num_targets ))\n",
    "for i in tqdm(range(instances)):\n",
    "    y_pred = model.predict([std_valid_spectra,std_valid_radii])\n",
    "    y_pred_valid[i] += y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b676e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_valid = y_pred_valid.reshape(-1,num_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecca2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform them back to original space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede0cd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_valid_org = transform_back(y_pred_valid,targets_mean[None, ...], targets_std[None, ...])\n",
    "y_pred_valid_org = y_pred_valid_org.reshape(instances, len(std_valid_spectra), num_targets)\n",
    "y_pred_valid_org = np.swapaxes(y_pred_valid_org, 1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f9c7f",
   "metadata": {},
   "source": [
    "### Evaluate - light track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb9b3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2932ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the ground truth\n",
    "GT_Quartiles_path = os.path.join(training_GT_path, 'QuartilesTable.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec0266",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_qs = load_Quartile_Table(GT_Quartiles_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_GT_Quartiles = all_qs[valid_index]\n",
    "valid_GT_Quartiles = np.swapaxes(valid_GT_Quartiles, 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00b1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract relevant quariltes from trace data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e9ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_pred_valid, q2_pred_valid, q3_pred_valid = np.quantile(y_pred_valid_org, [0.16,0.5,0.84],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8454c45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put them into correct format\n",
    "valid_q_pred = np.concatenate([q1_pred_valid[None,...], q2_pred_valid[None,...], q3_pred_valid[None,...]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a16d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate!\n",
    "light_track_metric(valid_GT_Quartiles, valid_q_pred, k =1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72996cd",
   "metadata": {},
   "source": [
    "### Evaluate - regular track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0104c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read trace and quartiles table \n",
    "GT_trace_path = os.path.join(training_GT_path, 'Tracedata.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d6ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming each distribution produce the same number of trace (N_samples)\n",
    "trace1_matrix = y_pred_valid_org\n",
    "# assuming uniform weight, and the weights must sum to 1\n",
    "trace1_weights_matrix = np.ones((trace1_matrix.shape[0], trace1_matrix.shape[1]))/trace1_matrix.shape[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623e9a8a",
   "metadata": {},
   "source": [
    "calculate the score. Note here that the GT trace data argument requires only the path to the tracedata.hdf5 file. It will open on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da1bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_calculate(trace1_matrix, trace1_weights_matrix, GT_trace_path, id_order = valid_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4423bb",
   "metadata": {},
   "source": [
    "## Generate prediction for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8488fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_test_data = h5py.File(os.path.join(test_path,'SpectralData.hdf5'),\"r\")\n",
    "aux_test_data = pd.read_csv(os.path.join(test_path,'AuxillaryTable.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725d88ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_spec_matrix = to_observed_matrix(spec_test_data,aux_test_data )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ce08e4",
   "metadata": {},
   "source": [
    "### same pre-processing as before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e932a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_test_spectra = standardise(test_spec_matrix[:,:,1], global_mean, global_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ee734",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_radii = aux_test_data[['star_radius_m', 'planet_radius_m']]\n",
    "## we would prefer to use Rsol and Rjup \n",
    "test_radii['star_radius'] = test_radii['star_radius_m']/RSOL\n",
    "test_radii['planet_radius'] = test_radii['planet_radius_m']/RJUP\n",
    "test_radii = test_radii.drop(['star_radius_m', 'planet_radius_m'],axis=1)\n",
    "\n",
    "std_test_radii= standardise(test_radii, mean_radii, stdev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47661bea",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e55168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = N_samples\n",
    "y_pred_distribution = np.zeros((instances, len(std_test_spectra), num_targets ))\n",
    "for i in tqdm(range(instances)):\n",
    "    y_pred = model.predict([std_test_spectra,test_radii])\n",
    "    y_pred_distribution[i] += y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbf4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_distribution = y_pred_distribution.reshape(-1,num_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c42ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## project back to original space\n",
    "y_pred_org = transform_back(y_pred_distribution,targets_mean[None, ...], targets_std[None, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf94222",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_org = y_pred_org.reshape(instances, len(std_test_spectra), num_targets)\n",
    "y_pred_org = np.swapaxes(y_pred_org, 1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03019da",
   "metadata": {},
   "source": [
    "## Package output into desired format\n",
    "We follow specific formats in the competition, to help make the process as painless as possible, we have included a few helper functions to make sure you have the right format in place for the submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c00b2b",
   "metadata": {},
   "source": [
    "### Light Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e47667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract quartiles estimate for 16th, 50th and 84th percentile.\n",
    "all_q1_pred, all_q2_pred, all_q3_pred = np.quantile(y_pred_org, [0.16,0.5,0.84],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a3bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LT_submission = to_light_track_format(all_q1_pred, all_q2_pred, all_q3_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6639b5d4",
   "metadata": {},
   "source": [
    "### Regular Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a4aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracedata = y_pred_org\n",
    "# weight takes into account the importance of each point in the tracedata. \n",
    "# Currently they are all equally weighted and thus I have created a dummy array that sums the contribution to 1\n",
    "weight = np.ones((y_pred_org.shape[0],y_pred_org.shape[1]))/np.sum(np.ones(y_pred_org.shape[1]) )\n",
    "\n",
    "RT_submission = to_regular_track_format(y_pred_org, \n",
    "                                        weight, \n",
    "                                        name=\"RT_submission.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d9a33",
   "metadata": {},
   "source": [
    "## check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732c0f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "LT_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b7071",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"RT_submission.hdf5\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['Planet_0']['tracedata'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec52ac61",
   "metadata": {},
   "source": [
    "## Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6808d3c3",
   "metadata": {},
   "source": [
    "There are different direction to take from here on, let us summarise the shortcoming of this model:\n",
    "- The data preprocessing is quite simplitic and could have invested more efforts.\n",
    "- we have only used 5000 data points, instead of the full dataset\n",
    "- we didnt train the model with results from the retrieval (QuartilesTable.csv for Light Track and Tracedata.hdf5 for Regular Track), which are the GT for this competition.\n",
    "- The conditional distribution from MCDropout is very restricted and Gaussian-like\n",
    "- So far we havent considered the atmospheric targets as a joint distribution\n",
    "- We have only used stellar radius and planet radius from the auxillary information\n",
    "- We have not done any hyperparameter tuning \n",
    "- the train test split here is not clean, as in, we split the data after we have augmented the data, which results in information leakage to the validation data. There is no leakage to the test data though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112940ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
